#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
awakenFlash_streaming_benchmark.py
Pure Non-Logic Streaming Benchmark (no XGBoost)
Measures: accuracy, f1, speed, memory, adaptivity, stability, interpretability
Author: auto-generated by assistant for user
"""

import os
import time
import math
import json
import shutil
import random
import tracemalloc
from dataclasses import dataclass, asdict

import numpy as np
import pandas as pd
from scipy.stats import entropy
from sklearn.datasets import fetch_openml
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score, f1_score
from sklearn.linear_model import SGDClassifier
from sklearn.utils import shuffle as sk_shuffle

# -------------------------
# Utility / Config
# -------------------------
OUT_DIR = "benchmark_results"
os.makedirs(OUT_DIR, exist_ok=True)
RANDOM_SEED = 42
np.random.seed(RANDOM_SEED)
random.seed(RANDOM_SEED)

@dataclass
class ChunkMetrics:
    chunk: int
    acc: float
    f1: float
    time_fit: float
    mem_current_mb: float
    mem_peak_mb: float
    adaptivity_score: float
    interpretability_entropy: float

# -------------------------
# Simple "Non-Logic" Engine
# (non-logic principles: warm-start, random features, RLS-like update,
#  self-teacher soft targets, adaptive forgetting)
# -------------------------
class AwakenFlash:
    def __init__(self, D=512, ridge=1.0, forgetting=0.995, teacher_warmup=2):
        """
        D: random-feature dimension
        ridge: regularization for closed-form solve
        forgetting: exponential forgetting factor for streaming updates
        teacher_warmup: number of chunks before self-teacher starts providing soft labels
        """
        self.D = D
        self.ridge = ridge
        self.forgetting = forgetting
        self.teacher_warmup = teacher_warmup

        self.W = None  # random projection (D/2 x n_features)
        self.alpha = None  # weights (D x K)
        self.classes_ = None
        self.class_to_idx = {}
        self.chunk_count = 0
        self.teacher_probs = None  # self-teacher probabilistic outputs (from previous internals)
        # internal moving baseline for adaptivity measurement
        self.recent_acc = []
        self.mem_snapshot = []

    def _ensure_W(self, n_features):
        rows = self.D // 2
        if self.W is None or self.W.shape[1] != n_features:
            rng = np.random.default_rng(RANDOM_SEED)
            scale = 1.0 / math.sqrt(max(1, n_features))
            self.W = rng.normal(0, scale, (rows, n_features)).astype(np.float32)

    def _rff(self, X):
        # Random Fourier-ish features (cos + sin)
        proj = X.astype(np.float32) @ self.W.T
        phi = np.hstack([np.cos(proj), np.sin(proj)]) * np.sqrt(2.0 / self.D)
        return phi  # shape (n, D)

    def _encode(self, y):
        if self.classes_ is None:
            self.classes_ = np.unique(y)
            self.class_to_idx = {c: i for i, c in enumerate(self.classes_)}
        return np.array([self.class_to_idx[val] for val in y], dtype=np.int32)

    def partial_fit(self, X, y, classes=None):
        """Streaming update: closed-form ridge with forgetting (approx RLS-ish)."""
        self.chunk_count += 1
        if classes is not None:
            self.classes_ = classes
            self.class_to_idx = {c: i for i, c in enumerate(classes)}

        n, n_features = X.shape
        self._ensure_W(n_features)
        phi = self._rff(X)  # (n, D)
        K = len(self.classes_)

        # generate soft targets: if teacher ready, blend with previous soft outputs
        y_idx = self._encode(y)
        y_onehot = np.zeros((n, K), dtype=np.float32)
        y_onehot[np.arange(n), y_idx] = 1.0

        # self-teacher: if we have alpha, generate probs and blend a bit
        if self.alpha is not None and self.chunk_count >= self.teacher_warmup:
            logits = phi @ self.alpha
            probs = np.exp(logits - logits.max(axis=1, keepdims=True))
            probs = probs / probs.sum(axis=1, keepdims=True)
            # Blend: favor ground truth but allow soft distillation
            alpha_teacher = 0.25
            y_onehot = (1 - alpha_teacher) * y_onehot + alpha_teacher * probs

        # normalize phi for numeric stability
        mu = phi.mean(axis=0, keepdims=True)
        std = phi.std(axis=0, keepdims=True) + 1e-8
        phi_norm = (phi - mu) / std

        # compute batch-level sufficient stats
        PhiT_Phi = (phi_norm.T @ phi_norm) / max(1, n)
        PhiT_y = (phi_norm.T @ y_onehot) / max(1, n)

        # forgetting: combine old alpha contribution
        ridge = self.ridge / max(1, n)
        H = PhiT_Phi + np.eye(phi_norm.shape[1]) * ridge

        if self.alpha is None:
            # initialize alpha (D x K)
            try:
                self.alpha = np.linalg.solve(H + 1e-6 * np.eye(H.shape[0]), PhiT_y)
            except np.linalg.LinAlgError:
                self.alpha = np.linalg.pinv(H + 1e-6 * np.eye(H.shape[0])) @ PhiT_y
        else:
            # online-style blend: forget old stats partially, then solve
            blend = self.forgetting
            rhs = PhiT_y + blend * self.alpha  # incorporate previous belief
            try:
                self.alpha = np.linalg.solve(H + 1e-6 * np.eye(H.shape[0]), rhs)
            except np.linalg.LinAlgError:
                self.alpha = np.linalg.pinv(H + 1e-6 * np.eye(H.shape[0])) @ rhs

        # keep small memory of recent accuracy for adaptivity checks
        return self

    def predict(self, X):
        if self.alpha is None:
            # fallback predict majority class if known
            if self.classes_ is None:
                return np.zeros(len(X), dtype=int)
            else:
                maj = self.classes_[0]
                return np.full(len(X), maj)
        phi = self._rff(X)
        # normalize with alpha's implied normalization -- approximate by column z-score
        mu = phi.mean(axis=0, keepdims=True)
        std = phi.std(axis=0, keepdims=True) + 1e-8
        phi_norm = (phi - mu) / std
        scores = phi_norm @ self.alpha  # (n, K)
        preds_idx = np.argmax(scores, axis=1)
        return self.classes_[preds_idx]

    def interpretability_entropy(self):
        "Compute entropy of normalized absolute weights as a proxy for interpretability"
        if self.alpha is None:
            return float("nan")
        # aggregate across classes
        w = np.abs(self.alpha).sum(axis=1)  # shape (D,)
        p = w / (w.sum() + 1e-12)
        return entropy(p + 1e-12)  # higher entropy = less interpretable

# -------------------------
# Data loader + drift simulator
# -------------------------
def load_covtype_subsample(n_total=80000, n_features_limit=None):
    """
    Loads; uses OpenML 'covertype' if available locally; otherwise attempts fetch_openml.
    For user-run: if fetch_openml is slow, replace with local dataset.
    """
    print("Loading dataset (OpenML covtype, may take a while)...")
    ds = fetch_openml(name="covertype", version=1, as_frame=False)
    X = ds.data.astype(np.float32)
    y = ds.target.astype(int)  # targets 1..7 usually
    y = y - y.min()  # convert to 0-based
    if n_features_limit:
        X = X[:, :n_features_limit]
    if len(X) > n_total:
        X, y = X[:n_total], y[:n_total]
    # shuffle and standardize
    X, y = sk_shuffle(X, y, random_state=RANDOM_SEED)
    scaler = StandardScaler()
    X = scaler.fit_transform(X)
    return X, y

def make_chunks(X, y, n_chunks=10, chunk_size=None):
    if chunk_size is None:
        chunk_size = len(X) // n_chunks
    chunks = []
    idx = 0
    for i in range(n_chunks):
        chunks.append((X[idx:idx+chunk_size], y[idx:idx+chunk_size]))
        idx += chunk_size
    return chunks

def inject_drift(X_chunk, mode="sudden", intensity=0.5, rng=None):
    """
    Drift injection:
      - sudden: permute feature columns partially and add noise
      - gradual: slowly add scaled noise
    intensity: 0..1
    """
    rng = rng or np.random.default_rng()
    X = X_chunk.copy()
    n, d = X.shape
    if mode == "sudden":
        # swap subset of columns
        k = max(1, int(d * intensity * 0.3))
        cols = rng.choice(d, k, replace=False)
        perm = rng.permutation(k)
        X[:, cols] = X[:, cols[perm]]
        # add noise
        X += rng.normal(0, 0.1 * intensity, size=X.shape)
    elif mode == "gradual":
        X += rng.normal(0, 0.02 * intensity, size=X.shape)
    return X

# -------------------------
# Benchmark runner
# -------------------------
def run_streaming_benchmark(n_chunks=10, chunk_size=4000, drift_schedule=None):
    """
    drift_schedule: dict mapping chunk index (1-based) -> {"mode":..., "intensity":...}
    """
    X, y = load_covtype_subsample(n_total=n_chunks * chunk_size)
    chunks = make_chunks(X, y, n_chunks=n_chunks, chunk_size=chunk_size)

    # instantiate model
    model = AwakenFlash(D=512, ridge=1.0, forgetting=0.98, teacher_warmup=2)

    # tracer
    tracemalloc.start()
    results = []

    # baseline for adaptivity: track drop then recovery shape
    window_acc = []

    for i, (Xc, yc) in enumerate(chunks, 1):
        print(f"\n--- Chunk {i}/{n_chunks} ---")
        rng = np.random.default_rng(RANDOM_SEED + i)

        # possibly inject drift
        if drift_schedule and i in drift_schedule:
            cfg = drift_schedule[i]
            Xc = inject_drift(Xc, mode=cfg.get("mode", "sudden"), intensity=cfg.get("intensity", 0.6), rng=rng)
            print(f"Injected drift at chunk {i}: {cfg}")

        # split
        split = int(0.8 * len(Xc))
        X_train, X_test = Xc[:split], Xc[split:]
        y_train, y_test = yc[:split], yc[split:]

        # fit + time + mem snapshot
        t0 = time.time()
        snapshot_before = tracemalloc.get_traced_memory()
        model.partial_fit(X_train, y_train, classes=np.unique(y))
        t_fit = time.time() - t0
        current, peak = tracemalloc.get_traced_memory()
        mem_current_mb = current / 1024 / 1024
        mem_peak_mb = peak / 1024 / 1024

        # predict + metrics
        t0 = time.time()
        y_pred = model.predict(X_test)
        t_pred = time.time() - t0
        acc = float(accuracy_score(y_test, y_pred))
        f1 = float(f1_score(y_test, y_pred, average="weighted", zero_division=0))

        # adaptivity: measure how quickly accuracy recovers after last drop (simple heuristic)
        window_acc.append(acc)
        # look back last 3 chunks
        lookback = window_acc[-4:]
        if len(lookback) >= 2:
            # adaptivity_score: ratio of recent increase relative to drop magnitude (clamped)
            drop = max(0.0, max(lookback) - min(lookback))
            recovery = 0.0
            if len(lookback) >= 2:
                recovery = lookback[-1] - lookback[0]
            adaptivity_score = float(np.tanh((recovery) / (drop + 1e-6)))
            adaptivity_score = max(0.0, adaptivity_score)
        else:
            adaptivity_score = 0.0

        # interpretability: entropy of alpha weight distribution
        ie = model.interpretability_entropy()

        # store
        metrics = ChunkMetrics(
            chunk=i,
            acc=acc,
            f1=f1,
            time_fit=t_fit + t_pred,
            mem_current_mb=mem_current_mb,
            mem_peak_mb=mem_peak_mb,
            adaptivity_score=adaptivity_score,
            interpretability_entropy=float(ie)
        )
        results.append(asdict(metrics))

        print(f" acc={acc:.4f} f1={f1:.4f} fit+pred_time={metrics.time_fit:.3f}s mem={metrics.mem_current_mb:.1f}/{metrics.mem_peak_mb:.1f}MB adapt={metrics.adaptivity_score:.3f} ie={metrics.interpretability_entropy:.3f}")

    tracemalloc.stop()
    df = pd.DataFrame(results)
    fn = os.path.join(OUT_DIR, "awakenFlash_streaming_results.csv")
    df.to_csv(fn, index=False)
    print(f"\nSaved results to {fn}")

    # summary
    summary = {
        "sun_mean_acc": float(df["acc"].mean()),
        "sun_mean_f1": float(df["f1"].mean()),
        "sun_mean_time": float(df["time_fit"].mean()),
        "sun_mem_mean_mb": float(df["mem_current_mb"].mean()),
        "sun_mem_peak_mb": float(df["mem_peak_mb"].max()),
        "sun_adaptivity_mean": float(df["adaptivity_score"].mean()),
        "sun_interpretability_mean": float(df["interpretability_entropy"].mean()),
        "sun_stability_var": float(df["acc"].var())
    }
    sfn = os.path.join(OUT_DIR, "awakenFlash_streaming_summary.json")
    with open(sfn, "w") as f:
        json.dump(summary, f, indent=2)
    print("Summary:", summary)
    return df, summary

# -------------------------
# Simple comparator utility (if you want to compare to XGB externally)
# -------------------------
def compare_to_xgboost_placeholder(df_nonlogic, xgb_summary=None):
    """
    The user asked to not use XGBoost for strength; this helper accepts an external xgb summary
    (dict with same metrics) and prints comparison. Keep as placeholder if you want to run
    XGBoost offline later and pass results in.
    """
    if xgb_summary is None:
        print("No XGBoost summary provided for comparison.")
        return
    nl = df_nonlogic
    print("Comparison (nonlogic vs xgb):")
    print(f" nonlogic mean acc: {nl['acc'].mean():.4f}  xgb acc: {xgb_summary.get('xgb_mean_acc'):.4f}")
    # ... extend as needed

# -------------------------
# Entrypoint
# -------------------------
if __name__ == "__main__":
    # Example drift schedule: sudden drift at chunk 4 and gradual drift at chunk 7
    drift_schedule = {
        4: {"mode": "sudden", "intensity": 0.8},
        7: {"mode": "gradual", "intensity": 0.6}
    }
    df, summary = run_streaming_benchmark(n_chunks=10, chunk_size=4000, drift_schedule=drift_schedule)
    print("\nDone.")
