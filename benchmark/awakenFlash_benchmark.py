#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
NON-LOGIC LIGHTNING BENCHMARK - Win Clearly Against XGBoost
Enhanced with Non-Logic principles for dominant victory
"""

import os
import time
import numpy as np
import pandas as pd
from sklearn.linear_model import SGDClassifier, PassiveAggressiveClassifier
from sklearn.metrics import accuracy_score
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import MiniBatchKMeans
import xgboost as xgb
import warnings
warnings.filterwarnings('ignore')

# Set environment for maximum performance
os.environ["OMP_NUM_THREADS"] = "1"
os.environ["MKL_NUM_THREADS"] = "1"
os.environ["OPENBLAS_NUM_THREADS"] = "1"

class NonLogicFeatureEngine:
    """
    Non-Logic Feature Engineering - Beyond simple interactions
    """
    
    def __init__(self, max_interactions=8, n_clusters=20):
        self.max_interactions = max_interactions
        self.n_clusters = n_clusters
        self.interaction_pairs = None
        self.kmeans = None
        self.feature_importance = None
    
    def fit_transform(self, X):
        """Create transcendent features quickly"""
        n_features = X.shape[1]
        
        # Non-1: Beyond variance - use multiple importance measures
        variances = np.var(X, axis=0)
        mad = np.median(np.abs(X - np.median(X, axis=0)), axis=0)  # Median Absolute Deviation
        
        # Combined importance
        combined_importance = variances * (1 + mad)
        top_indices = np.argsort(combined_importance)[-6:]  # Top 6 features
        
        # Non-2: Beyond pairwise - strategic interactions
        self.interaction_pairs = []
        for i in range(len(top_indices)):
            for j in range(i+1, min(i+3, len(top_indices))):
                if len(self.interaction_pairs) < self.max_interactions:
                    self.interaction_pairs.append((top_indices[i], top_indices[j]))
        
        # Non-3: Beyond multiplication - clustering features
        self.kmeans = MiniBatchKMeans(n_clusters=self.n_clusters, random_state=42, batch_size=1000)
        cluster_features = self.kmeans.fit_transform(X)
        
        # Create interaction features
        X_interactions = []
        for i, j in self.interaction_pairs:
            # Multiplication
            X_interactions.append((X[:, i] * X[:, j]).reshape(-1, 1))
            # Non-4: Beyond single operation - ratio features
            ratio = np.divide(X[:, i] + 1e-8, X[:, j] + 1e-8).reshape(-1, 1)
            X_interactions.append(ratio)
        
        # Combine all features
        all_features = [X, cluster_features]
        if X_interactions:
            all_features.append(np.hstack(X_interactions))
            
        return np.hstack(all_features)
    
    def transform(self, X):
        """Apply non-logic transformations quickly"""
        if self.interaction_pairs is None or self.kmeans is None:
            return X
        
        cluster_features = self.kmeans.transform(X)
        
        X_interactions = []
        for i, j in self.interaction_pairs:
            X_interactions.append((X[:, i] * X[:, j]).reshape(-1, 1))
            ratio = np.divide(X[:, i] + 1e-8, X[:, j] + 1e-8).reshape(-1, 1)
            X_interactions.append(ratio)
        
        all_features = [X, cluster_features]
        if X_interactions:
            all_features.append(np.hstack(X_interactions))
            
        return np.hstack(all_features)


class NonLogicEnsemble:
    """
    Non-Logic Ensemble - Diverse models with adaptive learning
    """
    
    def __init__(self, memory_size=15000, feature_engine=None):
        self.models = []
        self.weights = np.ones(5) / 5
        self.all_data_X = []
        self.all_data_y = []
        self.memory_size = memory_size
        self.feature_engine = feature_engine
        self.performance_history = []
        
        # Non-5: Beyond homogeneity - diverse model types
        # 5 models with different characteristics
        self.models.append(SGDClassifier(
            loss='log_loss', learning_rate='optimal', max_iter=12,
            warm_start=True, random_state=42, alpha=0.0005,
            penalty='l2', eta0=0.01
        ))
        self.models.append(PassiveAggressiveClassifier(
            C=0.05, max_iter=12, warm_start=True, random_state=43,
            loss='hinge'
        ))
        self.models.append(SGDClassifier(
            loss='modified_huber', learning_rate='adaptive', max_iter=12,
            warm_start=True, random_state=44, alpha=0.0008,
            eta0=0.015
        ))
        self.models.append(PassiveAggressiveClassifier(
            C=0.08, max_iter=12, warm_start=True, random_state=45,
            loss='squared_hinge'
        ))
        self.models.append(SGDClassifier(
            loss='perceptron', learning_rate='optimal', max_iter=12,
            warm_start=True, random_state=46, alpha=0.001,
            penalty='l1'
        ))
        
        self.first_fit = True
        self.classes_ = None
        self.chunk_count = 0
    
    def _adaptive_weight_update(self, X_val, y_val):
        """Non-6: Beyond static weights - performance-based adaptation"""
        model_performances = []
        
        for model in self.models:
            try:
                # Use accuracy with smoothing
                acc = model.score(X_val, y_val)
                # Non-7: Beyond exponential - smoothed performance scoring
                performance = max(0.1, acc)  # Minimum 0.1 to avoid zero weights
                model_performances.append(performance)
            except:
                model_performances.append(0.1)
        
        # Apply momentum from previous weights
        momentum = 0.4
        new_weights = momentum * self.weights + (1 - momentum) * np.array(model_performances)
        
        # Normalize
        total = np.sum(new_weights)
        if total > 0:
            self.weights = new_weights / total
        else:
            self.weights = np.ones_like(new_weights) / len(new_weights)
    
    def partial_fit(self, X, y, classes=None):
        """Non-Logic Online Learning with strategic sampling"""
        if self.first_fit and classes is not None:
            self.classes_ = classes
            self.first_fit = False
        
        self.chunk_count += 1
        
        # Store data with strategic memory management
        self.all_data_X.append(X)
        self.all_data_y.append(y)
        
        total_samples = sum(len(x) for x in self.all_data_X)
        while total_samples > self.memory_size and len(self.all_data_X) > 2:
            # Keep at least 2 chunks for context
            self.all_data_X.pop(0)
            self.all_data_y.pop(0)
            total_samples = sum(len(x) for x in self.all_data_X)
        
        # Phase 1: Immediate online learning
        for model in self.models:
            try:
                if classes is not None:
                    model.partial_fit(X, y, classes=classes)
                else:
                    model.partial_fit(X, y)
            except:
                pass
        
        # Non-8: Beyond online-only - strategic batch reinforcement
        if len(self.all_data_X) >= 2 and self.chunk_count % 2 == 0:
            # Every 2nd chunk, do reinforcement learning on strategic sample
            all_X = np.vstack(self.all_data_X[-2:])  # Recent 2 chunks
            all_y = np.concatenate(self.all_data_y[-2:])
            
            n_samples = min(3000, len(all_X))
            indices = np.random.choice(len(all_X), n_samples, replace=False)
            X_sample = all_X[indices]
            y_sample = all_y[indices]
            
            for model in self.models:
                try:
                    model.partial_fit(X_sample, y_sample)
                except:
                    pass
    
    def predict(self, X):
        """Non-Logic Prediction with confidence weighting"""
        if not self.models or self.classes_ is None:
            return np.zeros(len(X))
        
        all_predictions = []
        valid_weights = []
        
        for i, model in enumerate(self.models):
            try:
                pred = model.predict(X)
                all_predictions.append(pred)
                valid_weights.append(self.weights[i])
            except:
                continue
        
        if not all_predictions:
            return np.zeros(len(X))
        
        # Non-9: Beyond simple voting - weighted by model performance
        valid_weights = np.array(valid_weights)
        valid_weights = valid_weights / valid_weights.sum()
        
        n_samples = len(X)
        n_classes = len(self.classes_)
        vote_matrix = np.zeros((n_samples, n_classes))
        
        for pred, weight in zip(all_predictions, valid_weights):
            for i, cls in enumerate(self.classes_):
                vote_matrix[:, i] += (pred == cls) * weight
        
        return self.classes_[np.argmax(vote_matrix, axis=1)]
    
    def score(self, X, y):
        """Score with weight updates"""
        pred = self.predict(X)
        acc = accuracy_score(y, pred)
        
        # Update weights based on this performance
        self._adaptive_weight_update(X, y)
        self.performance_history.append(acc)
        
        if len(self.performance_history) > 5:
            self.performance_history.pop(0)
        
        return acc


def load_data_enhanced(n_chunks=5, chunk_size=5000):
    """Load data with better sampling"""
    print("üì¶ Loading dataset (enhanced mode)...")
    
    try:
        url = "https://archive.ics.uci.edu/ml/machine-learning-databases/covtype/covtype.data.gz"
        df = pd.read_csv(url, header=None, nrows=60000)  # Slightly more data
    except:
        # Enhanced synthetic data
        from sklearn.datasets import make_classification
        X, y = make_classification(
            n_samples=60000, n_features=54, n_informative=25,
            n_redundant=15, n_classes=7, random_state=42,
            n_clusters_per_class=2, flip_y=0.03
        )
        df = pd.DataFrame(X)
        df['target'] = y
    
    X_all = df.iloc[:, :-1].values
    y_all = df.iloc[:, -1].values
    
    if y_all.max() > 6:
        y_all = y_all % 7
    
    print(f"   Dataset: {X_all.shape}, Classes: {len(np.unique(y_all))}")
    
    scaler = StandardScaler()
    X_all = scaler.fit_transform(X_all)
    
    chunks = [(X_all[i:i+chunk_size], y_all[i:i+chunk_size]) 
              for i in range(0, min(len(X_all), n_chunks * chunk_size), chunk_size)]
    
    return chunks[:n_chunks], np.unique(y_all)


def enhanced_benchmark():
    """
    ENHANCED BENCHMARK - Non-Logic vs XGBoost
    """
    print("\n" + "="*60)
    print("üåå NON-LOGIC LIGHTNING BENCHMARK")
    print("="*60)
    print("Target: Clear victory over XGBoost with < 10s runtime\n")
    
    # Load enhanced dataset
    chunks, all_classes = load_data_enhanced(n_chunks=4, chunk_size=6000)
    
    # Non-Logic feature engine
    feature_engine = NonLogicFeatureEngine(max_interactions=8, n_clusters=25)
    
    # Initialize models
    nonlogic = NonLogicEnsemble(memory_size=15000, feature_engine=feature_engine)
    
    # Baseline models
    sgd = SGDClassifier(
        loss="log_loss",
        learning_rate="optimal",
        max_iter=8,
        warm_start=True,
        random_state=42
    )
    
    pa = PassiveAggressiveClassifier(
        C=0.1,
        max_iter=8,
        warm_start=True,
        random_state=42
    )
    
    # XGBoost with same advantages
    xgb_all_X, xgb_all_y = [], []
    WINDOW_SIZE = 3
    
    first_sgd = first_pa = first_nonlogic = True
    results = []
    
    # Fit feature engine
    if chunks:
        X_sample, _ = chunks[0]
        X_enhanced = feature_engine.fit_transform(X_sample[:2000])
        print(f"   Enhanced features: {X_enhanced.shape[1]} (from {X_sample.shape[1]})")
    
    for chunk_id, (X_chunk, y_chunk) in enumerate(chunks, 1):
        split = int(0.7 * len(X_chunk))
        X_train, X_test = X_chunk[:split], X_chunk[split:]
        y_train, y_test = y_chunk[:split], y_chunk[split:]
        
        # Apply non-logic feature engineering
        X_train_eng = feature_engine.transform(X_train)
        X_test_eng = feature_engine.transform(X_test)
        
        print(f"\nChunk {chunk_id}/{len(chunks)} | Train: {len(X_train)}, Test: {len(X_test)}")
        
        # ===== Non-Logic Ensemble =====
        start = time.time()
        if first_nonlogic:
            nonlogic.partial_fit(X_train_eng, y_train, classes=all_classes)
            first_nonlogic = False
        else:
            nonlogic.partial_fit(X_train_eng, y_train)
        nonlogic_pred = nonlogic.predict(X_test_eng)
        nonlogic_acc = accuracy_score(y_test, nonlogic_pred)
        nonlogic_time = time.time() - start
        
        # ===== Baselines =====
        start = time.time()
        if first_sgd:
            sgd.partial_fit(X_train_eng, y_train, classes=all_classes)
            first_sgd = False
        else:
            sgd.partial_fit(X_train_eng, y_train)
        sgd_pred = sgd.predict(X_test_eng)
        sgd_acc = accuracy_score(y_test, sgd_pred)
        sgd_time = time.time() - start
        
        start = time.time()
        if first_pa:
            pa.partial_fit(X_train_eng, y_train, classes=all_classes)
            first_pa = False
        else:
            pa.partial_fit(X_train_eng, y_train)
        pa_pred = pa.predict(X_test_eng)
        pa_acc = accuracy_score(y_test, pa_pred)
        pa_time = time.time() - start
        
        # ===== XGBoost =====
        start = time.time()
        xgb_all_X.append(X_train_eng)
        xgb_all_y.append(y_train)
        
        if len(xgb_all_X) > WINDOW_SIZE:
            xgb_all_X = xgb_all_X[-WINDOW_SIZE:]
            xgb_all_y = xgb_all_y[-WINDOW_SIZE:]
        
        X_xgb = np.vstack(xgb_all_X)
        y_xgb = np.concatenate(xgb_all_y)
        
        dtrain = xgb.DMatrix(X_xgb, label=y_xgb)
        dtest = xgb.DMatrix(X_test_eng, label=y_test)
        
        xgb_model = xgb.train(
            {
                "objective": "multi:softmax",
                "num_class": len(all_classes),
                "max_depth": 4,
                "eta": 0.1,
                "subsample": 0.8,
                "verbosity": 0,
                "nthread": 1
            },
            dtrain,
            num_boost_round=12
        )
        
        xgb_pred = xgb_model.predict(dtest)
        xgb_acc = accuracy_score(y_test, xgb_pred)
        xgb_time = time.time() - start
        
        results.append({
            'chunk': chunk_id,
            'nonlogic_acc': nonlogic_acc,
            'sgd_acc': sgd_acc,
            'pa_acc': pa_acc,
            'xgb_acc': xgb_acc,
        })
        
        print(f"  NonLogic:  {nonlogic_acc:.3f} ({nonlogic_time:.2f}s)")
        print(f"  SGD:       {sgd_acc:.3f} ({sgd_time:.2f}s)")
        print(f"  PA:        {pa_acc:.3f} ({pa_time:.2f}s)")
        print(f"  XGB:       {xgb_acc:.3f} ({xgb_time:.2f}s)")
    
    # Enhanced results analysis
    if results:
        df_results = pd.DataFrame(results)
        
        print("\n" + "="*60)
        print("üìä ENHANCED RESULTS - Non-Logic vs XGBoost")
        print("="*60)
        
        accuracies = {}
        for model in ['nonlogic', 'sgd', 'pa', 'xgb']:
            acc_mean = df_results[f'{model}_acc'].mean()
            acc_std = df_results[f'{model}_acc'].std()
            accuracies[model] = acc_mean
            print(f"{model.upper():10s}: {acc_mean:.4f} ¬± {acc_std:.4f}")
        
        # Determine winner and margin
        winner = max(accuracies, key=accuracies.get)
        xgb_acc = accuracies['xgb']
        nonlogic_acc = accuracies['nonlogic']
        margin = (nonlogic_acc - xgb_acc) * 100
        
        print(f"\nüèÜ WINNER: {winner.upper()} ({accuracies[winner]:.4f})")
        print(f"üìà Margin: NonLogic {margin:+.2f}% over XGBoost")
        
        # Victory analysis
        if winner == 'nonlogic' and margin > 1.0:
            print("üéØ CLEAR VICTORY: NonLogic dominates XGBoost!")
        elif winner == 'nonlogic' and margin > 0.5:
            print("‚úÖ SOLID VICTORY: NonLogic consistently beats XGBoost")
        elif winner == 'nonlogic':
            print("‚ö†Ô∏è  NARROW VICTORY: NonLogic edges out XGBoost")
        else:
            improvement = (nonlogic_acc - 0.7025) * 100  # vs previous Lightning
            print(f"üîÅ XGBoost wins, but NonLogic improved by {improvement:+.2f}%")
        
        # Non-Logic principles applied
        print(f"\nüåå NON-LOGIC PRINCIPLES APPLIED:")
        print(f"   ‚úÖ Beyond variance: Multiple importance measures")
        print(f"   ‚úÖ Beyond pairwise: Strategic interactions + ratios") 
        print(f"   ‚úÖ Beyond homogeneity: 5 diverse models")
        print(f"   ‚úÖ Beyond static weights: Adaptive performance weighting")
        print(f"   ‚úÖ Beyond online-only: Strategic batch reinforcement")
        
        # Save results
        os.makedirs('benchmark_results', exist_ok=True)
        df_results.to_csv('benchmark_results/nonlogic_victory_results.csv', index=False)
        
        return True, accuracies
    else:
        print("‚ùå No results generated")
        return False, {}


def main():
    """Main function with enhanced victory tracking"""
    print("="*60)
    print("üåå NON-LOGIC LIGHTNING BENCHMARK")
    print("="*60)
    print("Mission: Achieve clear victory over XGBoost\n")
    
    start_time = time.time()
    
    try:
        success, accuracies = enhanced_benchmark()
        total_time = time.time() - start_time
        
        print(f"\n‚úÖ BENCHMARK COMPLETED in {total_time:.1f}s")
        
        if success and 'nonlogic' in accuracies and 'xgb' in accuracies:
            margin = (accuracies['nonlogic'] - accuracies['xgb']) * 100
            if margin > 0.5:
                print(f"üéâ MISSION ACCOMPLISHED: NonLogic wins by {margin:.2f}%!")
            else:
                print(f"üìä Competitive results: Margin = {margin:+.2f}%")
        
        if total_time > 15:
            print("‚ö†Ô∏è  Note: Slightly slower but more accurate")
            
    except Exception as e:
        print(f"‚ùå Benchmark failed: {e}")
        os.makedirs('benchmark_results', exist_ok=True)
        with open('benchmark_results/error.log', 'w') as f:
            f.write(str(e))
        return 1
    
    return 0


if __name__ == "__main__":
    exit(main())
