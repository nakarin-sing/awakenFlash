#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
awakenFlash vs XGBoost ‚Äî Honest CI Benchmark (Speed Re-Validation)
"""

import time
import numpy as np
from sklearn.metrics import accuracy_score
from xgboost import XGBClassifier
import psutil, gc

# Import config ‡πÅ‡∏•‡∏∞ core functions ‡∏à‡∏≤‡∏Å‡πÑ‡∏ü‡∏•‡πå src.awakenFlash_core (‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡πÅ‡∏Å‡πâ N_FEATURES/N_CLASSES ‡πÅ‡∏•‡πâ‡∏ß)
try:
    from src.awakenFlash_core import train_step, infer, data_stream, N_SAMPLES, N_FEATURES, N_CLASSES, B, H, CONF_THRESHOLD, LS
except ImportError:
    # Fallback ‡∏Å‡∏£‡∏ì‡∏µ‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡∏£‡∏±‡∏ô‡πÉ‡∏ô‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á
    # (‡πÉ‡∏ô CI ‡∏à‡∏£‡∏¥‡∏á‡∏à‡∏∞‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏õ‡∏±‡∏ç‡∏´‡∏≤‡∏ô‡∏µ‡πâ)
    print("Error: Could not import core module. Using dummy config/data.")
    N_SAMPLES = 100000; N_FEATURES = 40; N_CLASSES = 3; H = 448; CONF_THRESHOLD = 80; LS = 0.006
    # ... (‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏µ‡∏Å‡∏≤‡∏£‡∏Å‡∏≥‡∏´‡∏ô‡∏î train_step, infer, data_stream, H, B, etc. ‡πÉ‡∏ô‡∏ó‡∏µ‡πà‡∏ô‡∏µ‡πâ‡∏´‡∏≤‡∏Å‡∏£‡∏±‡∏ô‡πÅ‡∏¢‡∏Å) ...

print("==============================")
print("Starting awakenFlash Benchmark (SPEED RE-VALIDATION N_F=40)")
print("==============================")

proc = psutil.Process()

# ---------------------------
# Prepare Data (‡πÉ‡∏ä‡πâ N_FEATURES ‡πÉ‡∏´‡∏°‡πà)
# ---------------------------
# Note: ‡πÉ‡∏ä‡πâ data_stream ‡∏ó‡∏µ‡πà‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÅ‡∏ö‡∏ö‡∏™‡∏∏‡πà‡∏°‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡πÄ‡∏î‡∏¥‡∏°
t_data_start = time.time()
X_train, y_train = next(data_stream(N_SAMPLES))
X_test, y_test = next(data_stream(100_000))
t_data_end = time.time()
print(f"Data Generation Time: {t_data_end - t_data_start:.2f}s")


# ---------------------------
# XGBoost Baseline
# ---------------------------
t0 = time.time()
xgb = XGBClassifier(n_estimators=30, max_depth=4, n_jobs=-1, random_state=42, tree_method='hist', verbosity=0)
xgb.fit(X_train, y_train)
xgb_train_time = time.time() - t0

# Inference Time
t0 = time.time()
xgb_pred = xgb.predict(X_test)
xgb_inf_ms = (time.time() - t0) / len(X_test) * 1000
xgb_acc = accuracy_score(y_test, xgb_pred)

del xgb
gc.collect()

# ---------------------------
# awakenFlash Real Run
# ---------------------------
scale = max(1.0, np.max(np.abs(X_train)) / 127.0)
X_i8 = np.clip(np.round(X_train / scale), -128, 127).astype(np.int8)

# ‡πÉ‡∏ä‡πâ N_FEATURES ‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á
mask = np.random.rand(N_FEATURES, H) < 0.7
values = np.random.randint(-4, 5, size=mask.sum()).astype(np.int8)
rows, cols = np.where(mask)
col_indices = cols.astype(np.int32)
indptr = np.zeros(H + 1, np.int32)
np.cumsum(np.bincount(rows, minlength=H), out=indptr[1:]) # ‡πÉ‡∏ä‡πâ rows ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì indptr (‡∏ï‡∏≤‡∏°‡πÇ‡∏Ñ‡πâ‡∏î v1.5)
b1 = np.zeros(H, np.int32)
W2 = np.random.randint(-4, 5, (H, N_CLASSES), np.int8)
b2 = np.zeros(N_CLASSES, np.int32)

t0 = time.time()
# Note: train_step ‡πÉ‡∏ô‡πÇ‡∏Ñ‡πâ‡∏î‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡πÉ‡∏ä‡πâ B ‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡∏±‡∏ß‡πÅ‡∏ö‡πà‡∏á batch ‡πÅ‡∏ï‡πà‡∏£‡∏±‡∏ô‡πÅ‡∏ö‡∏ö per-sample (prange n)
values, b1, W2, b2 = train_step(X_i8, y_train, values, col_indices, indptr, b1, W2, b2, H, CONF_THRESHOLD, LS)
awaken_train_time = time.time() - t0

X_test_i8 = np.clip(np.round(X_test / scale), -128, 127).astype(np.int8)
# Warm-up (‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡∏Å‡∏≤‡∏£‡∏ß‡∏±‡∏î Inference ‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥)
infer(X_test_i8[:1], values, col_indices, indptr, b1, W2, b2, H, CONF_THRESHOLD) 
t0 = time.time()
pred, ee_ratio = infer(X_test_i8, values, col_indices, indptr, b1, W2, b2, H, CONF_THRESHOLD)
awaken_inf_ms = (time.time() - t0) / len(X_test_i8) * 1000
awaken_acc = accuracy_score(y_test, pred)

# ---------------------------
# Results
# ---------------------------
# ... (‡∏™‡πà‡∏ß‡∏ô‡∏Å‡∏≤‡∏£‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡πÄ‡∏î‡∏¥‡∏°) ...
print("\n[XGBoost Results]")
print(f"Accuracy               : {xgb_acc:.4f}")
print(f"Train Time (s)         : {xgb_train_time:.2f}")
print(f"Inference (ms/sample)  : {xgb_inf_ms:.4f}")

print("\n[awakenFlash Results]")
print(f"Accuracy               : {awaken_acc:.4f}")
print(f"Train Time (s)         : {awaken_train_time:.2f}")
print(f"Inference (ms/sample)  : {awaken_inf_ms:.4f}")
print(f"Early Exit Ratio       : {ee_ratio*100:.1f}%")

print("\n==============================")
print("üèÅ FINAL VERDICT (N_FEATURES=40)")
print("==============================")

def verdict(name, awaken_val, xgb_val, higher_is_better=True):
    if awaken_val == xgb_val:
        return "ü§ù Tie"
    better = awaken_val > xgb_val if higher_is_better else awaken_val < xgb_val
    symbol = "‚úÖ awakenFlash wins" if better else "‚ùå XGBoost wins"
    diff = awaken_val - xgb_val if higher_is_better else xgb_val - awaken_val
    # ‡∏Å‡∏≤‡∏£‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏• Win Ratio ‡∏ó‡∏µ‡πà‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô‡∏Ç‡∏∂‡πâ‡∏ô
    win_ratio = (xgb_val / awaken_val) if not higher_is_better and awaken_val > 0 else (awaken_val / xgb_val)
    if not higher_is_better and awaken_val < xgb_val:
        return f"{symbol} ({win_ratio:.0f}x faster)"
    return f"{symbol} ({diff:+.4f})"

print(f"Accuracy Result        : {verdict('Accuracy', awaken_acc, xgb_acc)}")
print(f"Train Speed Result     : {verdict('Train', awaken_train_time, xgb_train_time, higher_is_better=False)}")
print(f"Inference Speed Result : {verdict('Inference', awaken_inf_ms, xgb_inf_ms, higher_is_better=False)}")
print("==============================")
