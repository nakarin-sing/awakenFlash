#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ULTIMATE STREAMING BENCHMARK - TRUE REAL-WORLD v1.1 (Extreme Speed Check)
Target: Ensemble Prediction Latency < 0.039ms (10x faster than XGBoost 0.39ms)

Key Changes (v1.1):
1. Ensemble Model: Changed from RandomForestClassifier(n_estimators=50) to ExtraTreesClassifier(n_estimators=5). (Extreme Latency Reduction)
2. n_batches: Reduced from 10 to 5 for smaller, quicker training batches.
3. Added n_jobs=-1 to maximize parallelization (crucial for tree-based models)
"""

import numpy as np
import pandas as pd
from sklearn.datasets import load_breast_cancer, load_iris, load_wine
from sklearn.model_selection import train_test_split
# FIX V1.1: Import ExtraTreesClassifier for speed optimization
from sklearn.ensemble import ExtraTreesClassifier 
from sklearn.metrics import accuracy_score
import xgboost as xgb
import time
import warnings
warnings.filterwarnings('ignore')

class TrustworthyBenchmark:
    def __init__(self):
        self.results = {}
        
    def validate_consistency(self, intermediate_scores, final_scores, tolerance=0.05):
        """‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏≠‡∏î‡∏Ñ‡∏•‡πâ‡∏≠‡∏á‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏Å‡∏•‡∏≤‡∏á‡∏ó‡∏≤‡∏á‡πÅ‡∏•‡∏∞‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢"""
        for model_name in intermediate_scores:
            if model_name in final_scores:
                # Use the last intermediate score, which is the final batch's score
                last_intermediate = intermediate_scores[model_name][-1]
                final = final_scores[model_name]
                if abs(last_intermediate - final) > tolerance:
                    print(f"‚ö†Ô∏è  ‡∏Ñ‡∏≥‡πÄ‡∏ï‡∏∑‡∏≠‡∏ô: {model_name} ‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡πÑ‡∏°‡πà‡∏™‡∏≠‡∏î‡∏Ñ‡∏•‡πâ‡∏≠‡∏á (‡∏Å‡∏•‡∏≤‡∏á‡∏ó‡∏≤‡∏á: {last_intermediate:.4f}, ‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢: {final:.4f})")
                    return False
        return True
    
    def measure_latency(self, model, X_test, n_repeats=50): # Increased repeats for better median stability
        """‡∏ß‡∏±‡∏î latency ‡πÅ‡∏ö‡∏ö‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥‡∏î‡πâ‡∏ß‡∏¢‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏ã‡πâ‡∏≥‡∏´‡∏•‡∏≤‡∏¢‡∏Ñ‡∏£‡∏±‡πâ‡∏á"""
        latencies = []
        for _ in range(n_repeats):
            start_time = time.time()
            model.predict(X_test)
            end_time = time.time()
            latencies.append((end_time - start_time) * 1000)  # convert to ms
        
        # ‡πÉ‡∏ä‡πâ‡∏Ñ‡πà‡∏≤ median ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏•‡∏î‡∏ú‡∏•‡∏Å‡∏£‡∏∞‡∏ó‡∏ö‡∏à‡∏≤‡∏Å outlier
        return np.median(latencies)
    
    def run_dataset_benchmark(self, dataset_name, data_loader, data_multiplier=1):
        print(f"\n{'='*60}")
        print(f"Dataset: {dataset_name} (x{data_multiplier} data)")
        print(f"{'='*60}")
        
        # ‡πÇ‡∏´‡∏•‡∏î‡πÅ‡∏•‡∏∞‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
        X, y = data_loader(return_X_y=True)
        
        if data_multiplier > 1:
            X = np.vstack([X] * data_multiplier)
            y = np.hstack([y] * data_multiplier)
        
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=0.3, random_state=42, stratify=y
        )
        
        # FIX V1.1: ‡∏•‡∏î n_batches ‡πÄ‡∏´‡∏•‡∏∑‡∏≠ 5
        n_batches = 5
        batch_size = len(X_train) // n_batches
        batches = []
        for i in range(n_batches):
            start_idx = i * batch_size
            end_idx = start_idx + batch_size if i < n_batches - 1 else len(X_train)
            batches.append((X_train[start_idx:end_idx], y_train[start_idx:end_idx]))
        
        # FIX V1.1: ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÄ‡∏õ‡πá‡∏ô ExtraTreesClassifier(n_estimators=5, n_jobs=-1)
        ensemble_model = ExtraTreesClassifier(n_estimators=5, random_state=42, n_jobs=-1) 
        # XGBoost ‡∏Ñ‡∏á‡πÄ‡∏î‡∏¥‡∏°‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÄ‡∏õ‡πá‡∏ô Baseline ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢
        xgb_model = xgb.XGBClassifier(n_estimators=50, random_state=42, n_jobs=-1) 
        
        intermediate_scores = {'Ens': [], 'XGB': []}
        intermediate_latencies = {'Ens': [], 'XGB': []}
        
        # Streaming training and evaluation
        for i, (X_batch, y_batch) in enumerate(batches):
            # Train models incrementally
            start_time = time.time()
            ensemble_model.fit(X_batch, y_batch)
            ensemble_train_time = (time.time() - start_time) * 1000
            
            start_time = time.time()
            xgb_model.fit(X_batch, y_batch)
            xgb_train_time = (time.time() - start_time) * 1000
            
            # Predict and evaluate
            ens_pred = ensemble_model.predict(X_test)
            xgb_pred = xgb_model.predict(X_test)
            
            ens_acc = accuracy_score(y_test, ens_pred)
            xgb_acc = accuracy_score(y_test, xgb_pred)
            
            # Measure prediction latency
            ens_latency = self.measure_latency(ensemble_model, X_test)
            xgb_latency = self.measure_latency(xgb_model, X_test)
            
            intermediate_scores['Ens'].append(ens_acc)
            intermediate_latencies['Ens'].append(ens_latency)
            intermediate_latencies['XGB'].append(xgb_latency)
            
            print(f"Batch {i:2d} | Ens: {ens_acc:.4f}({ens_latency:.3f}ms) | " # FIX V1.1: Latency to 3 decimal places
                  f"XGB: {xgb_acc:.4f}({xgb_latency:.3f}ms) | "
                  f"Train: Ens={ensemble_train_time:.2f}ms, XGB={xgb_train_time:.2f}ms")
        
        # Final evaluation with proper validation
        final_ens_acc = accuracy_score(y_test, ensemble_model.predict(X_test))
        final_xgb_acc = accuracy_score(y_test, xgb_model.predict(X_test))
        
        final_scores = {'Ens': final_ens_acc, 'XGB': final_xgb_acc}
        
        # Validate consistency
        is_consistent = self.validate_consistency(intermediate_scores, final_scores)
        
        print(f"\nFinal Test: Ens={final_ens_acc:.4f}, XGB={final_xgb_acc:.4f}")
        if not is_consistent:
            print("üîç ‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡πÑ‡∏°‡πà‡∏™‡∏≠‡∏î‡∏Ñ‡∏•‡πâ‡∏≠‡∏á‡πÉ‡∏ô‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå - ‡∏Ñ‡∏ß‡∏£‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö methodology")
        
        # Calculate robust statistics
        avg_ens_latency = np.median(intermediate_latencies['Ens'])
        avg_xgb_latency = np.median(intermediate_latencies['XGB'])
        avg_ens_acc = np.mean(final_scores['Ens']) # Use final score for summary
        avg_xgb_acc = np.mean(final_scores['XGB'])
        
        return {
            'final_accuracy': final_scores,
            'avg_accuracy': {'Ens': avg_ens_acc, 'XGB': avg_xgb_acc},
            'avg_latency': {'Ens': avg_ens_latency, 'XGB': avg_xgb_latency},
            'consistency_check': is_consistent
        }

def main():
    benchmark = TrustworthyBenchmark()
    
    datasets = [
        ("BreastCancer", load_breast_cancer, 1),
        ("Iris", load_iris, 3),
        ("Wine", load_wine, 3)
    ]
    
    all_results = []
    
    for dataset_name, loader, multiplier in datasets:
        result = benchmark.run_dataset_benchmark(dataset_name, loader, multiplier)
        result['dataset'] = dataset_name
        all_results.append(result)
    
    # Summary with transparent calculations
    print(f"\n{'='*60}")
    print("EXTREME SPEED CHECK SUMMARY (v1.1)")
    print(f"{'='*60}")
    
    total_ens_acc = sum(r['final_accuracy']['Ens'] for r in all_results)
    total_xgb_acc = sum(r['final_accuracy']['XGB'] for r in all_results)
    total_ens_latency = sum(r['avg_latency']['Ens'] for r in all_results)
    total_xgb_latency = sum(r['avg_latency']['XGB'] for r in all_results)
    n_datasets = len(all_results)
    
    overall_ens_acc = total_ens_acc / n_datasets
    overall_xgb_acc = total_xgb_acc / n_datasets
    overall_ens_latency = total_ens_latency / n_datasets
    overall_xgb_latency = total_xgb_latency / n_datasets
    
    print(f"Overall Accuracy:  Ensemble (ExtraTrees) = {overall_ens_acc:.4f}, XGBoost = {overall_xgb_acc:.4f}")
    print(f"Overall Latency:   Ensemble (ExtraTrees) = {overall_ens_latency:.3f}ms, XGBoost = {overall_xgb_latency:.3f}ms")
    
    accuracy_diff = overall_ens_acc - overall_xgb_acc
    speed_ratio = overall_xgb_latency / overall_ens_latency if overall_ens_latency > 0 else 1
    
    print(f"\nPerformance Analysis:")
    print(f"Accuracy Difference: {accuracy_diff:+.4f} ({'Ensemble ‡∏î‡∏µ‡∏Å‡∏ß‡πà‡∏≤' if accuracy_diff > 0 else 'XGBoost ‡∏î‡∏µ‡∏Å‡∏ß‡πà‡∏≤'})")
    print(f"Speed Ratio: {speed_ratio:.1f}x ({'Ensemble ‡πÄ‡∏£‡πá‡∏ß‡∏Å‡∏ß‡πà‡∏≤' if speed_ratio > 1 else 'XGBoost ‡πÄ‡∏£‡πá‡∏ß‡∏Å‡∏ß‡πà‡∏≤'})")
    
    consistent_count = sum(1 for r in all_results if r['consistency_check'])
    print(f"\nData Quality Check: {consistent_count}/{len(all_results)} datasets ‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏≠‡∏î‡∏Ñ‡∏•‡πâ‡∏≠‡∏á")
    
    if overall_ens_latency < overall_xgb_latency / 10:
        print("üéâ **GOAL ACHIEVED!** Ensemble ‡∏ä‡∏ô‡∏∞‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏£‡πá‡∏ß 10x")

if __name__ == "__main__":
    main()
